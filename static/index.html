<!DOCTYPE html>
<html lang="zh">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¼€æ”¾å¼LLMè™šæ‹Ÿä¸»æ’­</title>

    <!-- pixi live2d dependencies -->
    <!-- åŠ è½½Cubismå’ŒPixiJS -->
    <script src="libs/live2dcubismcore.min.js"></script>
    <script src="libs/live2d.min.js"></script>
    <script src="libs/pixi.min.js"></script>
    <script src="libs/index.min.js"></script>

    <script src="TaskQueue.js"></script>
    <script src="AudioMotionManager.js"></script>

    <!-- è¯­éŸ³æ¿€æ´»æ£€æµ‹ -->
    <script src="libs/ort.js"></script>
    <script src="libs/bundle.min.js"></script>

    <link rel="stylesheet" href="tuzi.css">
</head>

<body>
<div id="snow"></div>
<div id="background"></div>
    <div class="top-left">
        <button id="wsStatus">æœªè¿æ¥</button>
        <button id="stateDisplay">çŠ¶æ€ï¼šæ­£åœ¨åŠ è½½</button>
        <button id="pointerInteractionBtn">ğŸ‘€ é¼ æ ‡äº¤äº’å·²å¯ç”¨</button>
        <input type="text" id="wsUrl" placeholder="WebSocketåœ°å€">
        <div class="sensitivity-container">
            <span class="sensitivity-label">è¯­éŸ³æ£€æµ‹é˜ˆå€¼ï¼š</span>
            <input type="number" id="speechProbThreshold" min="1" max="100" value="97" title="è¯­éŸ³æ£€æµ‹å¯ä¿¡åº¦é˜ˆå€¼(%)">
        </div>
        <select id="configDropdown" aria-label="é…ç½®é€‰æ‹©">
            <option value="">é€‰æ‹©é…ç½®</option>
        </select>
        <select id="bgDropdown" aria-label="èƒŒæ™¯é€‰æ‹©">
            <option value="">é€‰æ‹©èƒŒæ™¯</option>
        </select>
        <select id="expressionSelect">
         <option value="">Select Expression</option>
        </select>
    </div>

    <canvas id="canvas"></canvas>

    <div class="bottom-container">
        <div class="fixed-bottom">
            <div id="message"></div>
            <input type="text" id="textInput" class="hidden" placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„æ¶ˆæ¯...">
        </div>
        <div class="control-buttons">
            <button id="micToggle">ğŸ™ï¸éº¦å…‹é£å·²å¼€å¯</button>
            <button id="interruptBtn">âŒè¯­éŸ³æ‰“æ–­å·²å…³é—­</button>
            <button id="sendText">ğŸ“æ˜¾ç¤ºæ–‡æœ¬è¾“å…¥</button>
        </div>
    </div>

    <script src="./live2d.js"></script>

    <script>
        // çŠ¶æ€è¯´æ˜ï¼š
        // idleï¼šLLMå¤„äºç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
        // thinking-speakingï¼šLLMæ­£åœ¨æ€è€ƒæˆ–è®²è¯ä¸­ã€‚
        // interruptedï¼šLLMæ­£åœ¨è¢«ç”¨æˆ·æ‰“æ–­ã€‚
        let state = "idle";
        let audioPlayer = new Audio();
        let voiceInterruptionOn = false;
        let fullResponse = ""; // å½“å‰å¯¹è¯é“¾ä¸­çš„å®Œæ•´å“åº”æ–‡æœ¬
        const audioManager = new AudioMotionManager();
        const stateDisplay = document.getElementById('stateDisplay');

        function updateStateDisplay() {
            stateDisplay.textContent = `çŠ¶æ€ï¼š${state}`;
        }

        function setState(newState) {
            state = newState;
            updateStateDisplay();
        }

        function interrupt() {
            console.log("ğŸ˜¡ğŸ‘ æ­£åœ¨æ‰“æ–­å¯¹è¯é“¾");
            console.log("å‘é€ä¸­: " + JSON.stringify({ type: "interrupt-signal", text: fullResponse }))
            ws.send(JSON.stringify({ type: "interrupt-signal", text: fullResponse }));
            setState("interrupted");
            model2.stopSpeaking();
            audioTaskQueue.clearQueue();
            console.log("å·²æ‰“æ–­ï¼");
        }

        let myvad;
        let previousTriggeredProbability = 0;
        let speechProbThreshold = document.getElementById('speechProbThreshold');

        window.addEventListener('load', function() {
            const savedThreshold = localStorage.getItem('speechProbThreshold');
            if (savedThreshold) {
                speechProbThreshold.value = savedThreshold;
            }

            const savedBackground = localStorage.getItem('selectedBackground');
            if (savedBackground) {
                setTimeout(() => {
                    bgDropdown.value = savedBackground;
                    document.getElementById("background").style.backgroundImage = `url('./bg/${savedBackground}')`;
                }, 1000);
            }
        });

        async function init_vad() {
            myvad = await vad.MicVAD.new({
                preSpeechPadFrames: 20,
                positiveSpeechThreshold: speechProbThreshold.value / 100,
                onSpeechStart: () => {
                    console.log("æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹: " + previousTriggeredProbability);
                    if (state === "thinking-speaking") {
                        interrupt();
                    } else {
                        console.log("ğŸ˜€ğŸ‘ æœªæ‰“æ–­ï¼Œæ­£å¸¸å¯¹è¯");
                    }
                },
                onFrameProcessed: (probs) => {
                    if (probs["isSpeech"] > previousTriggeredProbability) {
                        previousTriggeredProbability = probs["isSpeech"];
                    }
                },
                onVADMisfire: () => {
                    console.log("VADæ•…éšœã€‚LLMæ— æ³•å¬åˆ°æ‚¨çš„å£°éŸ³ã€‚");
                    if (state === "interrupted") {
                        state = "idle";
                    }
                    document.getElementById("message").textContent = "LLMæ— æ³•å¬åˆ°æ‚¨çš„å£°éŸ³ã€‚";
                },
                onSpeechEnd: (audio) => {
                    // å½“è¯´è¯ç»“æŸæ—¶è§¦å‘
                    audioTaskQueue.clearQueue();

                    if (!voiceInterruptionOn) {
                        stop_mic();
                    }

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        sendAudioPartition(audio);
                    }
                }
            });
        }

        speechProbThreshold.addEventListener('change', async function() {
            localStorage.setItem('speechProbThreshold', this.value);

            if (myvad) {
                await myvad.pause();
                await init_vad();
                if (micToggleState) {
                    await myvad.start();
                } else {
                    await myvad.pause();
                }
            }
        });

        const chunkSize = 4096;
        async function sendAudioPartition(audio) {
            console.log(audio)
            // æŒ‰å—å‘é€éŸ³é¢‘æ•°æ®åˆ°åç«¯
            for (let index = 0; index < audio.length; index += chunkSize) {
                const endIndex = Math.min(index + chunkSize, audio.length);
                const chunk = audio.slice(index, endIndex);
                ws.send(JSON.stringify({ type: "mic-audio-data", audio: chunk }));
            }
            ws.send(JSON.stringify({ type: "mic-audio-end" }));
        }

        let ws;
        const wsStatus = document.getElementById('wsStatus');
        const wsUrl = document.getElementById('wsUrl');
        const interruptBtn = document.getElementById('interruptBtn');
        const micToggle = document.getElementById('micToggle');
        const configDropdown = document.getElementById('configDropdown');
        const bgDropdown = document.getElementById('bgDropdown');

        wsUrl.value = "ws://127.0.0.1:12393/client-ws";
        // å¦‚æœåœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œ
        if (window.location.protocol.startsWith("http")) {
            console.log("æ­£åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­è¿è¡Œ");
            wsUrl.value = "/client-ws";
        } else {
            console.log("æ­£åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œ");
        }

        function connectWebSocket() {
            ws = new WebSocket(wsUrl.value);

            ws.onopen = function () {
                setState("idle");
                console.log("å·²è¿æ¥åˆ°WebSocket");
                wsStatus.textContent = "å·²è¿æ¥";
                wsStatus.classList.add('connected');
                fetchConfigurations();
                fetchBackgrounds();
            };

            ws.onclose = function () {
                setState("idle");
                console.log("WebSocketè¿æ¥å·²æ–­å¼€");
                wsStatus.textContent = "æœªè¿æ¥";
                wsStatus.classList.remove('connected');
                taskQueue.clearQueue();
            };

            ws.onmessage = function (event) {
                handleMessage(JSON.parse(event.data));
            };
        }

        wsStatus.addEventListener('click', connectWebSocket);

        function handleMessage(message) {
            console.log("æ”¶åˆ°åç«¯æ¶ˆæ¯: \n", message);
            switch (message.type) {
                case "full-text":
                    document.getElementById("message").textContent = message.text;
                    console.log("å®Œæ•´æ–‡æœ¬ï¼š", message.text);
                    break;
                case "control":
                    switch (message.text) {
                        case "start-mic":
                            start_mic();
                            break;
                        case "stop-mic":
                            stop_mic();
                            break;
                        case "conversation-chain-start":
                            setState("thinking-speaking");
                            fullResponse = "";
                            audioTaskQueue = new TaskQueue(20);
                            break;
                        case "conversation-chain-end":
                            setState("idle");
                            if (!voiceInterruptionOn) {
                                start_mic();
                            }
                            break;
                    }
                    break;
                case "expression":
                    setExpression(message.text);
                    break;
                case "mouth":
                    //setMouth(Number(message.text));
                    break;
                case "audio":
                    if (state == "interrupted") {
                        console.log("éŸ³é¢‘æ’­æ”¾è¢«æ‹¦æˆªã€‚å¥å­ï¼š", message.text);
                    } else {
                        addAudioTask(message.audio, message.volumes, message.slice_length, message.text, message.expressions);
                        console.log(message.expressions)
                        if(message.expressions === "sadness") {

                         setExpressionWithTimeout("å“­æ³£");
                        }else if(message.expressions === "smirk") {

                         setExpressionWithTimeout("è„¸çº¢");
                        }else if(message.expressions === "anger") {

                         setExpressionWithTimeout("ç”³è¯·");
                        }else if(message.expressions === "joy") {

                         setExpressionWithTimeout("è„¸çº¢");
                        }else if(message.expressions === "surprise") {

                         setExpressionWithTimeout("æ˜Ÿæ˜Ÿçœ¼");
                        }



                    }
                    break;
                case "set-model":
                    console.log("è®¾ç½®æ¨¡å‹ï¼š", message.text);
                    live2dModule.init().then(() => {
                        live2dModule.loadModel(message.text);
                    });
                    break;
                case "listExpressions":
                    console.log(listSupportedExpressions());
                    break;
                case "config-files":
                    populateConfigDropdown(message.files);
                    break;
                case "config-switched":
                    console.log(message.message);
                    document.getElementById("message").textContent = "é…ç½®åˆ‡æ¢æˆåŠŸï¼";
                    setState("idle");

                    if (micStateBeforeConfigSwitch) {
                        start_mic();
                    }
                    micStateBeforeConfigSwitch = null;
                    break;
                case "background-files":
                    populateBgDropdown(message.files);
                    break;
                default:
                    console.error("æœªçŸ¥æ¶ˆæ¯ç±»å‹: " + message.type);
                    console.log(message);
            }
        }

        function fetchConfigurations() {
            ws.send(JSON.stringify({ type: "fetch-configs" }));
        }

        function fetchBackgrounds() {
            ws.send(JSON.stringify({ type: "fetch-backgrounds" }));
        }

        function populateConfigDropdown(files) {
            configDropdown.innerHTML = '<option value="">é€‰æ‹©é…ç½®</option>';
            files.forEach(file => {
                const option = document.createElement('option');
                option.value = file;
                option.textContent = file;
                configDropdown.appendChild(option);
            });
        }

        function populateBgDropdown(files) {
            bgDropdown.innerHTML = '<option value="">é€‰æ‹©èƒŒæ™¯</option>';
            files.forEach(file => {
                const option = document.createElement('option');
                option.value = file;
                option.textContent = file;
                bgDropdown.appendChild(option);
            });
        }

        configDropdown.addEventListener('change', function () {
            const selectedConfig = configDropdown.value;
            if (selectedConfig) {
                setState("switching-config");
                document.getElementById("message").textContent = "æ­£åœ¨åˆ‡æ¢é…ç½®...";
                micStateBeforeConfigSwitch = micToggleState;
                if (micToggleState) {
                    stop_mic();
                }

                interrupt();
                ws.send(JSON.stringify({ type: "switch-config", file: selectedConfig }));
            }
        });

        bgDropdown.addEventListener('change', function () {
            const selectedBg = bgDropdown.value;
            if (selectedBg) {
                document.getElementById("background").style.backgroundImage = `url('./bg/${selectedBg}')`;
                localStorage.setItem('selectedBackground', selectedBg);
            }
        });

        function setExpression(expressionIndex) {
            expressionIndex = parseInt(expressionIndex);
            model2.internalModel.motionManager.expressionManager.setExpression(expressionIndex);
            console.info(`>> [x] -> è®¾ç½®è¡¨æƒ…ä¸º: (${expressionIndex})`);
        }

        function listSupportedExpressions() {
            emoMap = model2.internalModel.motionManager.expressionManager.emotionMap;
            console.log(emoMap);
        }

        function setMouth(mouthY,mouthX) {
            if (typeof model2.internalModel.coreModel.setParameterValueById === 'function') {
                model2.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', mouthY);
                model2.internalModel.coreModel.setParameterValueById('ParamMouthForm', mouthX);
            } else {
                model2.internalModel.coreModel.setParamFloat('PARAM_MOUTH_OPEN_Y', mouthY);
                model2.internalModel.coreModel.setParamFloat('PARAM_MOUTH_FORM', mouthX);
            }
        }

        audioTaskQueue = new TaskQueue(20);
        async function addAudioTask(audio_base64, volumes, slice_length, text = null, expression_list = null) {
            console.log(`1. æ·»åŠ éŸ³é¢‘ä»»åŠ¡ ${text} åˆ°é˜Ÿåˆ—`);


            if (state === "interrupted") {
                console.log("å› è¢«æ‰“æ–­ï¼Œè·³è¿‡éŸ³é¢‘ä»»åŠ¡");
                return;
            }

            audioTaskQueue.addTask(() => {
                return new Promise((resolve, reject) => {
                    audioManager.playAudioLipSync2(audio_base64, volumes, slice_length, text, expression_list,true, onComplete=resolve);

                }).catch(error => {
                    console.log("éŸ³é¢‘ä»»åŠ¡é”™è¯¯:", error);

                });
            });
        }

        function playAudioLipSync(audio_base64, volumes, slice_length, text = null, expression_list = null, onComplete) {
            if (state === "interrupted") {
                console.error("éŸ³é¢‘æ’­æ”¾è¢«é˜»æ­¢ã€‚çŠ¶æ€ï¼š", state);
                onComplete();
                return;
            }

            fullResponse += text;

            if (text) {
                document.getElementById("message").textContent = text;
            }

            const displayExpression = expression_list ? expression_list[0] : null;
            //print(displayExpression)
            console.log("å¼€å§‹æ’­æ”¾éŸ³é¢‘ï¼š", text);

            try {
                model2.speak("data:audio/wav;base64," + audio_base64, {
                    expression: displayExpression,
                    resetExpression: true,
                    onFinish: () => {
                        console.log("è¯­éŸ³æ’­æ”¾ç»“æŸ");
                        onComplete();
                    },
                    onError: (error) => {
                        console.error("éŸ³é¢‘æ’­æ”¾é”™è¯¯:", error);
                        onComplete();
                    }
                });
            } catch (error) {
                console.error("speakå‡½æ•°é”™è¯¯:", error);
                onComplete();
            }
        }
        function playAudioLipSync2(audio_base64, volumes, slice_length, text = null, expression_list = null, onComplete) {
            if (state === "interrupted") {
                console.error("éŸ³é¢‘æ’­æ”¾è¢«é˜»æ­¢ã€‚çŠ¶æ€ï¼š", state);
                onComplete();
                return;
            }

            fullResponse += text;

            if (text) {
                document.getElementById("message").textContent = text;
            }

            const displayExpression = expression_list ? expression_list[0] : null;
            //print(displayExpression)
            console.log("å¼€å§‹æ’­æ”¾éŸ³é¢‘ï¼š", text);

            try {
                model2.internalModel.motionManager.speak("data:audio/wav;base64," + audio_base64, {
                    expression: displayExpression,
                    resetExpression: true,
                    onFinish: () => {
                        console.log("è¯­éŸ³æ’­æ”¾ç»“æŸ");
                        onComplete();
                    },
                    onError: (error) => {
                        console.error("éŸ³é¢‘æ’­æ”¾é”™è¯¯:", error);
                        onComplete();
                    }
                });
            } catch (error) {
                console.error("speakå‡½æ•°é”™è¯¯:", error);
                onComplete();
            }
        }
        async function start_mic() {
            try {
                if (myvad == null) {
                    await init_vad();
                }
                console.log("éº¦å…‹é£å¼€å§‹ç›‘å¬");
                await myvad.start();
                micToggleState = true;
                micToggle.textContent = "ğŸ™ï¸éº¦å…‹é£å·²å¼€å¯";
            } catch (error) {
                console.error("éº¦å…‹é£å¯åŠ¨å¤±è´¥:", error);
                micToggleState = false;
                micToggle.textContent = "âŒéº¦å…‹é£å·²å…³é—­";
            }
        }

        function stop_mic() {
            console.log("éº¦å…‹é£åœæ­¢ç›‘å¬");
            if (myvad) {
                myvad.pause();
            }
            micToggleState = false;
            micToggle.textContent = "âŒéº¦å…‹é£å·²å…³é—­";
        }

        interruptBtn.addEventListener('click', function () {
            voiceInterruptionOn = !voiceInterruptionOn;
            interruptBtn.textContent = voiceInterruptionOn ? "ğŸ–ï¸è¯­éŸ³æ‰“æ–­å·²å¼€å¯" : "âŒè¯­éŸ³æ‰“æ–­å·²å…³é—­";
        });

        let micToggleState = true;
        micToggle.addEventListener('click', function () {
            micToggleState ? stop_mic() : start_mic();
        });

        connectWebSocket();

        const textInput = document.getElementById('textInput');
        const sendText = document.getElementById('sendText');

        function handleTextSubmit() {
            const text = textInput.value;
            if (text && ws && ws.readyState === WebSocket.OPEN) {
                if (state === "thinking-speaking") {
                    interrupt();
                } else {
                    console.log("ğŸ˜€ğŸ‘ æœªæ‰“æ–­ï¼Œæ­£å¸¸å¯¹è¯");
                }
                ws.send(JSON.stringify({
                    type: "text-input",
                    text: text
                }));
                textInput.value = '';
                audioTaskQueue.clearQueue();
            }
        }

        let textInputVisible = false;
        sendText.addEventListener('click', function() {
            textInputVisible = !textInputVisible;
            textInput.classList.toggle('hidden');
            sendText.textContent = textInputVisible ? "ğŸ“éšè—æ–‡æœ¬è¾“å…¥" : "ğŸ“æ˜¾ç¤ºæ–‡æœ¬è¾“å…¥";
        });

        textInput.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                handleTextSubmit();
            }
        });
    </script>
</body>

</html>
